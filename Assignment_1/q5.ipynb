{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "abbdef96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2b8fffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import load_synth \n",
    "from data import load_mnist\n",
    "from data import download_mnist\n",
    "from data import save_mnist\n",
    "from data import init\n",
    "from data import load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b2a9a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation functions\n",
    "def sigmoid_array(z): \n",
    "    sigmoid = 1.0/(1.0 + np.exp(-z))\n",
    "    return sigmoid \n",
    "\n",
    "def softmax_array(z):\n",
    "    output = (np.exp(z)/np.exp(z).sum())\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e00ee5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test), num_cls = load_mnist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "910bc446",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_tensor(z):\n",
    "    factor = np.linalg.norm(z)\n",
    "    normalized = z/factor\n",
    "    \n",
    "    return normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e04d2bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_x_train = normalize_tensor(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "229557ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters_tensor(x_input, hidden_nodes=300, output_labels=10):\n",
    "    \n",
    "    attributes = x_input.shape[1]\n",
    "\n",
    "    W = np.random.normal(0, 1, size=(attributes, hidden_nodes))\n",
    "    b = np.random.normal(0, 1, hidden_nodes)\n",
    "    b = np.expand_dims(b, axis=1)\n",
    "    V = np.random.normal(0, 1, size=(hidden_nodes, output_labels))\n",
    "    c = np.random.normal(0, 1, output_labels)\n",
    "    c = np.expand_dims(c, axis=1)\n",
    "    \n",
    "    weights = {'W': W, 'b': b, 'V':V, 'c': c}\n",
    "    \n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "36478a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_prop_vector(x, W, b, V, c):\n",
    "    \"\"\"\n",
    "    forward propagation for a two layer neural network\n",
    "    \n",
    "    Arguments:\n",
    "    x - input\n",
    "    weights -- \n",
    "        W - weights, layer 1\n",
    "        b - bias, layer 1\n",
    "        V - weights, layer 2\n",
    "        c - bias, layer 2\n",
    "    Return: parameters\n",
    "    \"\"\"\n",
    "    x = np.expand_dims(x, axis=1)\n",
    "    #print('x:', x.shape)\n",
    "    k = np.dot(W.T, x) + b\n",
    "    #print('k:', k.shape)\n",
    "    h = sigmoid_array(k)\n",
    "    #print('h:', h.shape)\n",
    "    o = np.dot(V.T, h) + c\n",
    "    #print('o:', o.shape)\n",
    "    yhat = softmax_array(o) \n",
    "    #print('yhat:', yhat.shape)\n",
    "    parameters = {'k': k, 'h': h, 'o': o, 'yhat': yhat}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "18e153d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(ypred, y):\n",
    "    \n",
    "    loss = -(np.log(ypred[y]))\n",
    "    \n",
    "    return loss/float(ypred.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0679674a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_loss(yhat, y):\n",
    "    y_vector = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "    y_vector[y] = 1\n",
    "    y_vector = np.expand_dims(y_vector, axis=1)\n",
    "    \n",
    "    loss = -np.sum(np.dot(y_vector.T, np.log(yhat)))\n",
    "    \n",
    "    return loss/float(yhat.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1fc1ff3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_derivative(x):\n",
    "    sig_d = np.dot(sigmoid_array(x).T, (1-sigmoid_array(x)))\n",
    "    return(sig_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d91c2dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_prop_vector(x, W, b, V, c, y, parameters):\n",
    "    \n",
    "    o = parameters['o']\n",
    "    h = parameters['h']\n",
    "    k = parameters['k']\n",
    "    yhat = parameters['yhat']\n",
    "    \n",
    "    x = np.expand_dims(x, axis=1)\n",
    "    # one-hot vector\n",
    "    one_hot_vector = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "    one_hot_vector[y] = 1\n",
    "    one_hot_vector = np.expand_dims(one_hot_vector, axis=1) \n",
    "\n",
    "    grad_o = yhat - one_hot_vector\n",
    "    do_dV = h\n",
    "    grad_V = np.dot(do_dV, grad_o.T)\n",
    "    grad_c = grad_o\n",
    "    do_dh = V\n",
    "    grad_h = np.dot(do_dh, grad_o)\n",
    "    dh_dk = sigmoid_derivative(k)\n",
    "    dh_dW = x\n",
    "    grad_W = np.dot(dh_dW, (dh_dk * grad_h).T)\n",
    "    grad_b = grad_h * dh_dk\n",
    "    gradients = {'grad_o': grad_o,\n",
    "                 'grad_V': grad_V,\n",
    "                 'grad_c': grad_c,\n",
    "                 'grad_h': grad_h,\n",
    "                 'grad_W': grad_W,\n",
    "                 'grad_b': grad_b,\n",
    "                }\n",
    "    \n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3d66d745",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_propagation(x, W, b, V, c, y):\n",
    "    \n",
    "    parameters = forward_prop_vector(x, W, b, V, c)\n",
    "    prediction = parameters['yhat']\n",
    "    cost = compute_loss(prediction, y)\n",
    "    gradients = back_prop_vector(x, W, b, V, c, y, parameters)\n",
    "    \n",
    "    return cost, gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4595bdbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "q5_weights = initialize_parameters_tensor(x_train, hidden_nodes=300, output_labels=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "74b98404",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1:  (784, 300)\n",
      "b1:  (300, 1)\n",
      "W2:  (300, 10)\n",
      "b2:  (10, 1)\n"
     ]
    }
   ],
   "source": [
    "W1 = q5_weights['W']\n",
    "print('W1: ', W1.shape)\n",
    "b1 = q5_weights['b']\n",
    "print('b1: ', b1.shape)\n",
    "W2 = q5_weights['V']\n",
    "print('W2: ', W2.shape)\n",
    "b2 = q5_weights['c']\n",
    "print('b2: ', b2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3232a3f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: (784, 1)\n",
      "k: (300, 1)\n",
      "h: (300, 1)\n",
      "o: (10, 1)\n",
      "yhat: (10, 1)\n"
     ]
    }
   ],
   "source": [
    "cost, grad = vector_propagation(normal_x_train[0], W1, b1, W2, b2, y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "98bcfb15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(x_set, W, b, V, c, y_set, lr, epochs):\n",
    "    loss_record = []\n",
    "    avg_loss_per_epoch = []\n",
    "    std_loss_per_epoch = []\n",
    "    \n",
    "    for num in range(epochs):\n",
    "        loss_per_epoch = []\n",
    "        for x, y in zip(x_set, y_set):\n",
    "            loss, grads = vector_propagation(x, W, b, V, c, y)\n",
    "\n",
    "            grad_W = grads['grad_W']\n",
    "            grad_b = grads['grad_b']\n",
    "            grad_V = grads['grad_V']\n",
    "            grad_c = grads['grad_c']\n",
    "            \n",
    "            # Updating weights:\n",
    "\n",
    "            W -= lr * grad_W\n",
    "            \n",
    "            b -= lr * grad_b\n",
    "            \n",
    "            V -= lr * grad_V\n",
    "            \n",
    "            c -= lr * grad_c\n",
    "            \n",
    "            weights = {'W': W, 'b': b, 'V': V, 'c': c}\n",
    "            loss_record.append(loss)\n",
    "            loss_per_epoch.append(loss)\n",
    "            \n",
    "        avg_loss_per_epoch.append(np.mean(loss_record))\n",
    "        std_loss_per_epoch.append(np.std(loss_record))\n",
    "        \n",
    "    return weights, grads, loss_record, avg_loss_per_epoch, std_loss_per_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13570cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "27e4fe4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights1, grads1, loss1, avg_loss, std_loss = gradient_descent(normal_x_train, W1, b1, W2, b2, y_train, 0.01, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ffc5cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885ee4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "std_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9118a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
